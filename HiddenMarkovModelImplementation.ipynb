{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hidden Markov Model: Tensor Flow Implementation   \n",
        "- Author: Alejandro Meza\n"
      ],
      "metadata": {
        "id": "8UHRlPcQ-Vz1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "\"The Hidden Markov Model is a finite set of states, each of which is associated with a (generally multidimensional) probability distribution. Transitions among the states are governed by a set of probabilities called transition probabilities.\" (http://jedlik.phy.bme.hu/~gerjanos/HMM/node4.html)\n",
        "\n",
        "\n",
        "A hidden Markov model (HMM) is a Markov model in which the observations are dependent on a latent (or \"hidden\") Markov process (referred to as X). An HMM requires that there be an observable process Y whose outcomes depend on the outcomes of X in a known way. Since X cannot be observed directly, the goal is to learn about state of X by observing Y.\n",
        "(https://en.wikipedia.org/wiki/Hidden_Markov_model)"
      ],
      "metadata": {
        "id": "ufenxTZG-mo8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basically, we're going to have a bunch of observations that depend on some hidden states. The value of a state X depends only on the value of state X-1 (the previous state). The goal is to obtain the hidden states that maximize the probability of seeing the current observation.\n"
      ],
      "metadata": {
        "id": "Bp4GGpDrLCI6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Important concepts\n",
        "\n",
        "\n",
        "- States: Represents different situations or conditions in the model.\n",
        "\n",
        "- Observations: Represents the observable outcomes associated with each state.\n",
        "\n",
        "- Initial Distribution: specifies the probabilities of starting in each state. I\n",
        "\n",
        "- Transition distribution: Defines the probabilities of transitioning from one state to another.\n",
        "\n",
        "- Observation distribution/emission probability: Describes the likelihood of observing a particular outcome given the current hidden state."
      ],
      "metadata": {
        "id": "kCHklyDy_L3d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Real case application\n",
        "\n",
        "One real application of the hidden markov model is the following:\n",
        "\n",
        "Hidden Markov Models (HMMs) have been widely used in Natural Language Processing (NLP), particularly for Part-of-Speech (POS) tagging. POS tagging involves assigning grammatical categories (such as noun, verb, adjective, etc.) to each word in a sentence. Here's how HMMs are applied in POS tagging:\n",
        "\n",
        "- Model Representation: in the context of POS tagging, an HMM can be represented with hidden states corresponding to POS tags (e.g., noun, verb) and observable events corresponding to words in a sentence.\n",
        "\n",
        "- Hidden States: hidden states in the HMM represent the underlying POS tags. For example, you may have hidden states like \"Noun,\" \"Verb,\" \"Adjective,\" etc.\n",
        "\n",
        "- Observations are the words in a given sentence. Each word is associated with a specific POS tag. The goal is to infer the most likely sequence of hidden states (POS tags) given the observed sequence of words.\n",
        "\n",
        "- Transition Probabilities: transition probabilities in the HMM represent the likelihood of transitioning from one POS tag to another. For example, the probability of transitioning from a noun to a verb.\n",
        "\n",
        "\n",
        "- Emission Probabilities: emission probabilities represent the likelihood of observing a specific word given a particular POS tag. For example, the probability of observing the word \"run\" given the POS tag \"Verb.\""
      ],
      "metadata": {
        "id": "4mNEzH5OMExr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After this, it is the moment to see how can we solve two different problems using **tensorflow**."
      ],
      "metadata": {
        "id": "SBZ8ybZxENPx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **USE CASE 1**  -Categorical Data-"
      ],
      "metadata": {
        "id": "mEGwsRG3BBXh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SOLVING WIKIPEDIA EXAMPLE\n",
        "\n",
        "Consider two friends, Alice and Bob, who live far apart from each other and who talk together daily over the telephone about what they did that day. Bob is only interested in three activities: walking in the park, shopping, and cleaning his apartment. The choice of what to do is determined exclusively by the weather on a given day. Alice has no definite information about the weather, but she knows general trends. Based on what Bob tells her he did each day, Alice tries to guess what the weather must have been like.\n",
        "\n",
        "Alice believes that the weather operates as a discrete Markov chain. There are two states, \"Rainy\" and \"Sunny\", but she cannot observe them directly, that is, they are hidden from her. On each day, there is a certain chance that Bob will perform one of the following activities, depending on the weather: \"walk\", \"shop\", or \"clean\". Since Bob tells Alice about his activities, those are the observations. The entire system is that of a hidden Markov model (HMM).\n",
        "\n",
        "Alice knows the general weather trends in the area, and what Bob likes to do on average. In other words, the parameters of the HMM are known. They can be represented as follows in Python:\n",
        "\n",
        "(https://en.wikipedia.org/wiki/Hidden_Markov_model)"
      ],
      "metadata": {
        "id": "-2-uc4boAe1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#REQUIREMENTS\n",
        "\n",
        "#Possible states based on the activities that Bob performs. Hidden Markov Process X\n",
        "states = (\"Rainy\", \"Sunny\")\n",
        "\n",
        "#Observations that atre vissible: Y\n",
        "observations = (\"walk\", \"shop\", \"clean\")\n",
        "\n",
        "#Weather trends\n",
        "start_probability = {\"Rainy\": 0.6, \"Sunny\": 0.4}\n",
        "\n",
        "transition_probability = {\n",
        "    \"Rainy\": {\"Rainy\": 0.7, \"Sunny\": 0.3},\n",
        "    \"Sunny\": {\"Rainy\": 0.4, \"Sunny\": 0.6},\n",
        "}\n",
        "\n",
        "emission_probability = {\n",
        "    \"Rainy\": {\"walk\": 0.1, \"shop\": 0.4, \"clean\": 0.5},\n",
        "    \"Sunny\": {\"walk\": 0.6, \"shop\": 0.3, \"clean\": 0.1},\n",
        "}"
      ],
      "metadata": {
        "id": "vwGzW2s8Atli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, in our case, Alice cannot observe the weather, but she knows the activities that Bob performs.\n",
        "\n",
        "Here, we have two hidden states: \"Rainy\" and \"Sunny.\" For each hidden state, there is a set of emission probabilities associated with the observable activities (\"walk\", \"shop\", \"clean\"). Each entry in the emission_probability dictionary provides the probability of observing a particular activity given the current hidden state.\n",
        "\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/HMMGraph.svg/1280px-HMMGraph.svg.png\" alt=\"Colab Image\" width=\"400\" height=\"300\">"
      ],
      "metadata": {
        "id": "v3obSxblA2Ka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Definition of our system\n",
        "Now, let's solve the problem!"
      ],
      "metadata": {
        "id": "sqSq00tDBLzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_probability as tfp  # We are using a different module from tensorflow this time\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "Ny3JvYiOBQOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfd = tfp.distributions  # making a shortcut for later on\n",
        "\n",
        "# 1) Define states and observations\n",
        "states = (\"Rainy\", \"Sunny\")\n",
        "activities = (\"walk\", \"shop\", \"clean\")\n",
        "\n",
        "# Initial distribution: probability for each state\n",
        "start_probability = {\"Rainy\": 0.6, \"Sunny\": 0.4}\n",
        "initial_distribution = tfd.Categorical(probs=[start_probability[state] for state in states])\n",
        "\n",
        "# 2) Transition distribution: probability of transitions between transitions\n",
        "transition_probability = {\n",
        "    \"Rainy\": {\"Rainy\": 0.7, \"Sunny\": 0.3},\n",
        "    \"Sunny\": {\"Rainy\": 0.4, \"Sunny\": 0.6},\n",
        "}\n",
        "transition_distribution_probs = [[transition_probability[from_state][to_state] for to_state in states] for from_state in states]\n",
        "transition_distribution = tfd.Categorical(probs=transition_distribution_probs)\n",
        "\n",
        "\n",
        "# 3) Observation distribution: values associated with each state and observation.\n",
        "#The probability of Y given a hidden X\n",
        "emission_probability = {\n",
        "    \"Rainy\": {\"walk\": 0.1, \"shop\": 0.4, \"clean\": 0.5},\n",
        "    \"Sunny\": {\"walk\": 0.6, \"shop\": 0.3, \"clean\": 0.1},\n",
        "}\n",
        "observation_distribution_probs = [[emission_probability[state][activity] for activity in activities] for state in states]\n",
        "observation_distribution = tfd.Categorical(probs=observation_distribution_probs)"
      ],
      "metadata": {
        "id": "UHEI43FhBSnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Test our system"
      ],
      "metadata": {
        "id": "8jx-5z-iBaL4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After defining our markov system, it's moment to test it, and make predictions with it!"
      ],
      "metadata": {
        "id": "WkAK0M6MBb7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a sample sequence\n",
        "num_steps = 10  # specify the length of the sequence\n",
        "hidden_states = [states[initial_distribution.sample().numpy()]]  # initialize with the initial state\n",
        "\n",
        "#predict next state, based on the transition distribution\n",
        "for _ in range(1, num_steps):\n",
        "    #generates a single sample, and convert this sample from a tensor to a numpy array.\n",
        "    #Finally, it obtains the value, and stores it in next_state variable\n",
        "    next_state = states[transition_distribution.sample(sample_shape=(1,)).numpy()[0][0]]\n",
        "    hidden_states.append(next_state)\n",
        "\n",
        "# Sample observations based on sampled hidden states\n",
        "observations = [activities[observation_distribution.sample(sample_shape=(1,)).numpy()[0][0]] for _ in range(num_steps)]\n",
        "\n",
        "# Print the sampled sequence in a readable format\n",
        "print(\"Hidden States:\")\n",
        "print(hidden_states)\n",
        "\n",
        "print(\"\\nObserved States:\")\n",
        "print(observations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YskrVaxBjQo",
        "outputId": "af765674-9f68-46f2-eefb-1b0c559b5e16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hidden States:\n",
            "['Rainy', 'Sunny', 'Rainy', 'Sunny', 'Rainy', 'Rainy', 'Rainy', 'Rainy', 'Rainy', 'Sunny']\n",
            "\n",
            "Observed States:\n",
            "['clean', 'clean', 'shop', 'shop', 'shop', 'shop', 'clean', 'clean', 'clean', 'clean']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And that's it! We have made a succesfull implementation of a markov model using tensorflow ^^"
      ],
      "metadata": {
        "id": "b0EaoVzTBo0n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **USE CASE 2**  -Discrete Data-\n",
        "\n",
        "We are going to try to predict the temperature given the following requisites:\n",
        "- Cold days are encoded by a 0 and hot days are encoded by a 1.\n",
        "- The first day in our sequence has an 80% chance of being cold.\n",
        "- A cold day has a 30% chance of being followed by a hot day.\n",
        "- A hot day has a 20% chance of being followed by a cold day.\n",
        "- On each day the temperature is normally distributed with mean and standard deviation 0 and 5 on a cold day and mean and standard deviation 15 and 10 on a hot day."
      ],
      "metadata": {
        "id": "rBxeQamyB2__"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the following material for this example:\n",
        "-  https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/HiddenMarkovModel  \n",
        "- https://colab.research.google.com/drive/15Cyy2H7nT40sGR7TBN5wBvgTd57mVKay#forceEdit=true&sandboxMode=true&scrollTo=ssOcn-nIOCcV"
      ],
      "metadata": {
        "id": "EYSh99AtDYht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definition of our system"
      ],
      "metadata": {
        "id": "O_O40u6gElhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfd = tfp.distributions\n",
        "\n",
        "# Initial distribution: probability for each state\n",
        "initial_distribution = tfd.Categorical(probs=[0.2, 0.8])\n",
        "\n",
        "# 2) Transition distribution: probability of transitions between transitions\n",
        "transition_distribution = tfd.Categorical(probs=[[0.7, 0.3], #cold: cold, hot\n",
        "                                                 [0.2, 0.8]])  #hot: cold, hot\n",
        "\n",
        "# 3) Observation distribution: values associated with each state and observation.\n",
        "#The probability of Y given a hidden X\n",
        "emission_probability = tfd.Normal(loc=[0., 15.], scale=[5.0, 10.0])"
      ],
      "metadata": {
        "id": "nbF_r9t7CQtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test our system"
      ],
      "metadata": {
        "id": "DOkKMrfcEqYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#num_steps: how far you want to predict ahead in time new states\n",
        "NUMBER_STATES_TO_PREDICT=7\n",
        "\n",
        "model = tfd.HiddenMarkovModel(\n",
        "    initial_distribution=initial_distribution,\n",
        "    transition_distribution=transition_distribution,\n",
        "    observation_distribution= emission_probability,\n",
        "    num_steps=NUMBER_STATES_TO_PREDICT)"
      ],
      "metadata": {
        "id": "gDCkzcUACeZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean = model.mean()\n",
        "\n",
        "# due to the way TensorFlow works on a lower level we need to evaluate part of the graph\n",
        "# from within a session to see the value of this tensor\n",
        "\n",
        "# in the new version of tensorflow we need to use tf.compat.v1.Session() rather than just tf.Session()\n",
        "with tf.compat.v1.Session() as sess:\n",
        "  print(mean.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKYxmmpRDSLj",
        "outputId": "6b41c8bc-b486-47b7-a61f-d566ae1d7759"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11.999999 10.500001  9.75      9.375     9.1875    9.09375   9.046875]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And that's it, we have performed another good example of this interesting model. If you're willing to know more, please check this online resources:\n",
        "\n",
        "- https://www.cs.ubc.ca/~murphyk/Bayes/rabiner.pdf\n",
        "- https://www.cs.sjsu.edu/~stamp/RUA/HMM.pdf\n",
        "- https://www.youtube.com/watch?v=fX5bYmnHqqE"
      ],
      "metadata": {
        "id": "8Sg1bjfLNygE"
      }
    }
  ]
}